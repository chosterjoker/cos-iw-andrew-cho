{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Andrew Cho's COS IW Project (Movie Recommendations)\n",
    "\n",
    "The following notebook implements:\n",
    "\n",
    "## Recommendation Models:\n",
    "1. Collaborative Filtering (CF) - Matrix factorization using SVD\n",
    "2. Content-Based Filtering (CBF) - Semantic embeddings from movie text\n",
    "3. Hybrid - switching combination (CF for warm items, CBF fallback for cold items)\n",
    "\n",
    "## Evaluation Regimes:\n",
    "- Warm: Standard per-user 80/20 temporal split on dataset\n",
    "- Cold Item: True item cold-start (items appearing for first time after temporal cutoff)\n",
    "- Few-shot Users: Users with limited history (k=1,3,5 ratings in training)\n",
    "\n",
    "## Metrics:\n",
    "- Regression: MAE, RMSE\n",
    "- Ranking: Recall@K, NDCG@K, Precision@K (with negative sampling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Usage: AI code generation was utilized, specifically for cache and dir path creation and implementation, evaluation framework testing, code modularization, function comments, clear print statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "1. Clone the repo.\n",
    "2. The github repo comes with a requirements.txt file. Create a virtual enviorment with those package versions installed then run this notebook using that virtual enviorment.\n",
    "3. Set up a folder called data in the same directory as this notebook and add the files from this link: \n",
    "4. Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split as surprise_split\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# variable configuration\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# paths to store intermediate results like figures, embeddings, etc.\n",
    "DATA_DIR = Path('data')\n",
    "OUTPUT_DIR = Path('outputs')\n",
    "TABLES_DIR = OUTPUT_DIR / 'tables'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "CACHE_DIR = OUTPUT_DIR / 'cache'\n",
    "\n",
    "# create output directories\n",
    "for d in [TABLES_DIR, FIGURES_DIR, CACHE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# experiment parameters\n",
    "K = 10  # Top-K for ranking metrics (Precision@10, Recall@10, NDCG@10)\n",
    "N_NEG = 100  # Number of negative samples per positive (for ranking evaluation)\n",
    "MAX_USERS_RANK = 5000  # Max users for ranking evaluation (samples from test set for efficiency)\n",
    "COLD_CUTOFF_QUANTILE = 0.8  # Temporal cutoff for cold-start split (~Oct 2018)\n",
    "RELEVANCE_THRESHOLD = 4.0  # Minimum rating to consider \"relevant\" (4.0 = \"liked it\")\n",
    "                           # items with rating ≥4.0 count as \"hits\" in ranking metrics\n",
    "\n",
    "# hybrid alpha values to sweep (including 1.0 for pure CF)\n",
    "ALPHA_VALUES = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ratings data\n",
    "ratings = pd.read_csv(DATA_DIR / 'ratings.csv')\n",
    "print(f\"Number of ratings: {len(ratings):,}\")\n",
    "\n",
    "# load movies data\n",
    "movies = pd.read_pickle(DATA_DIR / 'movies_enriched_with_semantic.pkl')\n",
    "print(f\"Number of movies: {len(movies):,}\")\n",
    "\n",
    "# select relevant columns - use semantic_text for CBF\n",
    "movies = movies[['movieId', 'semantic_text']].copy()\n",
    "movies.columns = ['movieId', 'text']\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate (userId, movieId) pairs\n",
    "n_before = len(ratings)\n",
    "ratings = ratings.drop_duplicates(subset=['userId', 'movieId'], keep='first')\n",
    "n_after = len(ratings)\n",
    "n_duplicates = n_before - n_after\n",
    "\n",
    "print(f\"duplicate pairs removed: {n_duplicates:,}\")\n",
    "if n_duplicates > 0:\n",
    "    print(f\"  before: {n_before:,}, after: {n_after:,}\")\n",
    "\n",
    "# basic stats\n",
    "n_ratings = len(ratings)\n",
    "n_users = ratings['userId'].nunique()\n",
    "n_items = ratings['movieId'].nunique()\n",
    "rating_min = ratings['rating'].min()\n",
    "rating_max = ratings['rating'].max()\n",
    "\n",
    "ts_min = ratings['timestamp'].min()\n",
    "ts_max = ratings['timestamp'].max()\n",
    "date_min = datetime.fromtimestamp(ts_min)\n",
    "date_max = datetime.fromtimestamp(ts_max)\n",
    "\n",
    "print(f\"\\nratings statistics:\")\n",
    "print(f\"total ratings: {n_ratings:,}\")\n",
    "print(f\"unique users: {n_users:,}\")\n",
    "print(f\"unique items: {n_items:,}\")\n",
    "print(f\"rating range: [{rating_min}, {rating_max}]\")\n",
    "print(f\"date range: {date_min.strftime('%Y-%m-%d')} to {date_max.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# sparsity\n",
    "total_possible = n_users * n_items\n",
    "sparsity = 1 - (n_ratings / total_possible)\n",
    "print(f\"sparsity: {sparsity*100:.4f}%\")\n",
    "\n",
    "# distribution\n",
    "ratings_per_user = ratings.groupby('userId').size()\n",
    "ratings_per_item = ratings.groupby('movieId').size()\n",
    "\n",
    "print(f\"ratings per user: mean={ratings_per_user.mean():.2f}, median={ratings_per_user.median():.0f}\")\n",
    "print(f\"ratings per item: mean={ratings_per_item.mean():.2f}, median={ratings_per_item.median():.0f}\")\n",
    "\n",
    "# check overlap\n",
    "rated_items = set(ratings['movieId'].unique())\n",
    "movie_items = set(movies['movieId'].unique())\n",
    "overlap = rated_items.intersection(movie_items)\n",
    "missing_in_movies = rated_items - movie_items\n",
    "\n",
    "print(f\"\\nmovies data check:\")\n",
    "print(f\"movies in file: {len(movie_items):,}\")\n",
    "print(f\"items with ratings: {len(rated_items):,}\")\n",
    "print(f\"overlap: {len(overlap):,}\")\n",
    "print(f\"items missing movie text: {len(missing_in_movies):,}\")\n",
    "\n",
    "# filter to items with movie text\n",
    "if len(missing_in_movies) > 0:\n",
    "    print(f\"\\nfiltering to items with movie text\")\n",
    "    ratings = ratings[ratings['movieId'].isin(movie_items)]\n",
    "    print(f\"  ratings after filter: {len(ratings):,}\")\n",
    "    n_users = ratings['userId'].nunique()\n",
    "    n_items = ratings['movieId'].nunique()\n",
    "    print(f\"  users: {n_users:,}, items: {n_items:,}\")\n",
    "\n",
    "# dtypes\n",
    "ratings['userId'] = ratings['userId'].astype(np.int32)\n",
    "ratings['movieId'] = ratings['movieId'].astype(np.int32)\n",
    "ratings['rating'] = ratings['rating'].astype(np.float32)\n",
    "ratings['timestamp'] = ratings['timestamp'].astype(np.int64)\n",
    "\n",
    "print(f\"\\ndata types verified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER UTILITIES\n",
    "\n",
    "def build_user_item_map(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Build user -> set of items mapping from a DataFrame.\"\"\"\n",
    "    user_items = defaultdict(set)\n",
    "    for uid, mid in zip(df['userId'].values, df['movieId'].values):\n",
    "        user_items[uid].add(mid)\n",
    "    return dict(user_items)\n",
    "\n",
    "\n",
    "def build_item_user_map(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Build item -> set of users mapping from a DataFrame.\"\"\"\n",
    "    item_users = defaultdict(set)\n",
    "    for uid, mid in zip(df['userId'].values, df['movieId'].values):\n",
    "        item_users[mid].add(uid)\n",
    "    return dict(item_users)\n",
    "\n",
    "\n",
    "def sample_negatives(candidate_pool: set, forbidden: set, n: int, rng: np.random.Generator) -> list:\n",
    "    \"\"\"\n",
    "    Sample n negatives from candidate_pool, excluding forbidden items.\n",
    "    \n",
    "    Args:\n",
    "        candidate_pool: set of all possible items to sample from\n",
    "        forbidden: set of items to exclude (user's train/test items)\n",
    "        n: number of negatives to sample\n",
    "        rng: numpy random generator for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        List of sampled negative item IDs\n",
    "    \"\"\"\n",
    "    available = list(candidate_pool - forbidden)\n",
    "    if len(available) < n:\n",
    "        return available  # Return all available if not enough\n",
    "    return list(rng.choice(available, size=n, replace=False))\n",
    "\n",
    "\n",
    "def create_id_mappings(items: np.ndarray):\n",
    "    \"\"\"Create bidirectional ID mappings for items.\"\"\"\n",
    "    unique_items = np.unique(items)\n",
    "    item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
    "    idx_to_item = {idx: item for item, idx in item_to_idx.items()}\n",
    "    return item_to_idx, idx_to_item\n",
    "\n",
    "\n",
    "def print_split_stats(train_df: pd.DataFrame, test_df: pd.DataFrame, name: str):\n",
    "    \"\"\"Print statistics for a train/test split.\"\"\"\n",
    "    print(f\"\\n{name} Split Statistics:\")\n",
    "    print(f\"  Train: {len(train_df):,} ratings, {train_df['userId'].nunique():,} users, {train_df['movieId'].nunique():,} items\")\n",
    "    print(f\"  Test:  {len(test_df):,} ratings, {test_df['userId'].nunique():,} users, {test_df['movieId'].nunique():,} items\")\n",
    "    \n",
    "    # check overlap\n",
    "    train_users = set(train_df['userId'].unique())\n",
    "    test_users = set(test_df['userId'].unique())\n",
    "    train_items = set(train_df['movieId'].unique())\n",
    "    test_items = set(test_df['movieId'].unique())\n",
    "    \n",
    "    print(f\"  Test users in train: {len(test_users.intersection(train_users)):,} / {len(test_users):,}\")\n",
    "    print(f\"  Test items in train: {len(test_items.intersection(train_items)):,} / {len(test_items):,}\")\n",
    "\n",
    "\n",
    "print(\"Helper utilities defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Builders (3 Evaluation Regimes)\n",
    "\n",
    "### 4.1 Warm Split (Per-user temporal 80/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 WARM SPLIT: Per-user temporal 80/20\n",
    "\n",
    "def build_warm_split(df: pd.DataFrame, train_ratio: float = 0.8):\n",
    "    \"\"\"\n",
    "    Create warm split: per-user temporal split.\n",
    "    First train_ratio% of each user's interactions -> train\n",
    "    Remaining -> test\n",
    "    \"\"\"\n",
    "    # sort by user and timestamp\n",
    "    df_sorted = df.sort_values(['userId', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    # compute cumulative count per user\n",
    "    df_sorted['user_cum_count'] = df_sorted.groupby('userId').cumcount() + 1\n",
    "    df_sorted['user_total'] = df_sorted.groupby('userId')['userId'].transform('count')\n",
    "    df_sorted['user_position'] = df_sorted['user_cum_count'] / df_sorted['user_total']\n",
    "    \n",
    "    # split based on position\n",
    "    train_mask = df_sorted['user_position'] <= train_ratio\n",
    "    \n",
    "    train_df = df_sorted[train_mask][['userId', 'movieId', 'rating', 'timestamp']].copy()\n",
    "    test_df = df_sorted[~train_mask][['userId', 'movieId', 'rating', 'timestamp']].copy()\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# build warm split\n",
    "print(\"building WARM split...\")\n",
    "train_warm, test_warm = build_warm_split(ratings, train_ratio=0.8)\n",
    "\n",
    "# print statistics\n",
    "print_split_stats(train_warm, test_warm, \"WARM\")\n",
    "\n",
    "# leakage check: verify timestamps don't leak\n",
    "# for warm split, each user's test timestamps should be >= train timestamps\n",
    "print(\"\\nLeakage Check (per-user temporal ordering):\")\n",
    "train_max_ts = train_warm.groupby('userId')['timestamp'].max()\n",
    "test_min_ts = test_warm.groupby('userId')['timestamp'].min()\n",
    "common_users = train_max_ts.index.intersection(test_min_ts.index)\n",
    "violations = (train_max_ts[common_users] > test_min_ts[common_users]).sum()\n",
    "print(f\"  Users with timestamp violations: {violations} / {len(common_users)}\")\n",
    "assert violations == 0, \"Timestamp leakage detected in warm split!\"\n",
    "print(\"  ✓ No timestamp leakage detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 True Item Cold-Start Split (Global temporal cutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 TRUE ITEM COLD-START SPLIT\n",
    "\n",
    "def build_cold_item_split(df: pd.DataFrame, q_cut: float = 0.8):\n",
    "    \"\"\"\n",
    "    Create true item cold-start split.\n",
    "    \n",
    "    1. Global temporal cutoff at q_cut quantile\n",
    "    2. Cold items = items that appear for the FIRST TIME after cutoff\n",
    "    3. test_cold = interactions with cold items after cutoff\n",
    "    \n",
    "    Args:\n",
    "        df: ratings DataFrame\n",
    "        q_cut: quantile for temporal cutoff (default 0.8 = 80th percentile)\n",
    "        \n",
    "    Returns:\n",
    "        train_cold: ratings before cutoff\n",
    "        test_cold: ratings after cutoff for cold items only\n",
    "        cold_items: set of cold item IDs\n",
    "        T: timestamp cutoff value\n",
    "    \"\"\"\n",
    "    # compute cutoff timestamp\n",
    "    T = df['timestamp'].quantile(q_cut)\n",
    "    T_date = datetime.fromtimestamp(T)\n",
    "    print(f\"Temporal cutoff T: {T} ({T_date.strftime('%Y-%m-%d')})\")\n",
    "    \n",
    "    # split by cutoff\n",
    "    train_cold = df[df['timestamp'] <= T].copy()\n",
    "    test_after = df[df['timestamp'] > T].copy()\n",
    "    \n",
    "    print(f\"Ratings before cutoff (train_cold): {len(train_cold):,}\")\n",
    "    print(f\"Ratings after cutoff: {len(test_after):,}\")\n",
    "    \n",
    "    # find first appearance of each item (over ALL data)\n",
    "    first_time_per_item = df.groupby('movieId')['timestamp'].min()\n",
    "    \n",
    "    # cold items: first appearance is AFTER cutoff T\n",
    "    cold_items = set(first_time_per_item[first_time_per_item > T].index)\n",
    "    print(f\"cold items (first appear after T): {len(cold_items):,}\")\n",
    "    \n",
    "    # test_cold: only cold items from test_after\n",
    "    test_cold = test_after[test_after['movieId'].isin(cold_items)].copy()\n",
    "    print(f\"cold item interactions in test: {len(test_cold):,}\")\n",
    "    print(f\"Users in test_cold: {test_cold['userId'].nunique():,}\")\n",
    "    \n",
    "    return train_cold, test_cold, cold_items, T\n",
    "\n",
    "\n",
    "# build cold item split\n",
    "print(\"building COLD ITEM split...\")\n",
    "train_cold, test_cold, cold_items, T_cutoff = build_cold_item_split(ratings, q_cut=COLD_CUTOFF_QUANTILE)\n",
    "\n",
    "# CRITICAL LEAKAGE CHECK\n",
    "train_cold_items = set(train_cold['movieId'].unique())\n",
    "leakage = cold_items.intersection(train_cold_items)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COLD ITEM LEAKAGE CHECK\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"cold items: {len(cold_items):,}\")\n",
    "print(f\"Train items: {len(train_cold_items):,}\")\n",
    "print(f\"Intersection (SHOULD BE ZERO): {len(leakage)}\")\n",
    "\n",
    "assert len(leakage) == 0, f\"LEAKAGE DETECTED! {len(leakage)} cold items found in training set!\"\n",
    "print(\"✓ No leakage: cold items are truly unseen in training\")\n",
    "\n",
    "print_split_stats(train_cold, test_cold, \"COLD ITEM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Few-shot User Splits (k=1, 3, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 FEW-SHOT USER SPLITS\n",
    "\n",
    "def build_fewshot_splits(df: pd.DataFrame, k_values: list = [1, 3, 5]):\n",
    "    \"\"\"\n",
    "    Create few-shot user splits for different k values.\n",
    "    \n",
    "    For each k:\n",
    "    - train_k = first k interactions per user (sorted by timestamp)\n",
    "    - test_k = remaining interactions\n",
    "    - Only include users with > k interactions\n",
    "    \n",
    "    Args:\n",
    "        df: ratings DataFrame\n",
    "        k_values: list of k values to create splits for\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary: {k: (train_k, test_k)}\n",
    "    \"\"\"\n",
    "    # sort by user and timestamp once\n",
    "    df_sorted = df.sort_values(['userId', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    # add position within each user\n",
    "    df_sorted['user_position'] = df_sorted.groupby('userId').cumcount() + 1\n",
    "    user_counts = df_sorted.groupby('userId').size()\n",
    "    \n",
    "    fewshot_splits = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\nBuilding few-shot split for k={k}...\")\n",
    "        \n",
    "        # filter to users with > k interactions\n",
    "        valid_users = user_counts[user_counts > k].index\n",
    "        df_valid = df_sorted[df_sorted['userId'].isin(valid_users)]\n",
    "        \n",
    "        # split: first k -> train, rest -> test\n",
    "        train_k = df_valid[df_valid['user_position'] <= k][['userId', 'movieId', 'rating', 'timestamp']].copy()\n",
    "        test_k = df_valid[df_valid['user_position'] > k][['userId', 'movieId', 'rating', 'timestamp']].copy()\n",
    "        \n",
    "        fewshot_splits[k] = (train_k, test_k)\n",
    "        \n",
    "        print(f\"  Users with > {k} interactions: {len(valid_users):,}\")\n",
    "        print_split_stats(train_k, test_k, f\"Few-shot k={k}\")\n",
    "    \n",
    "    return fewshot_splits\n",
    "\n",
    "\n",
    "# build few-shot splits\n",
    "print(\"building FEW-SHOT USER splits...\")\n",
    "fewshot_splits = build_fewshot_splits(ratings, k_values=[1, 3, 5])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FEW-SHOT SPLITS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for k, (train_k, test_k) in fewshot_splits.items():\n",
    "    print(f\"k={k}: Train={len(train_k):,}, Test={len(test_k):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Implementations\n",
    "\n",
    "### 5.1 Collaborative Filtering (CF) - Surprise SVD\n",
    "\n",
    "We use the Surprise library's SVD implementation which includes:\n",
    "- Biased matrix factorization (global mean, user bias, item bias)\n",
    "- Regularization\n",
    "- Optimized C++ backend for efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collaborative filtering using surprise SVD\n",
    "\n",
    "class CFModel:\n",
    "    # matrix factorization with user/item biases\n",
    "    \n",
    "    def __init__(self, n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=SEED):\n",
    "        self.n_factors = n_factors\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr_all = lr_all\n",
    "        self.reg_all = reg_all\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.train_items_set = None\n",
    "        self.train_users_set = None\n",
    "        self.global_mean = None\n",
    "        self.user_means = None\n",
    "        self.reader = None\n",
    "        \n",
    "    def fit(self, train_df: pd.DataFrame, verbose=True):\n",
    "        if verbose:\n",
    "            print(f\"fitting cf model (svd) with {self.n_factors} factors, {self.n_epochs} epochs\")\n",
    "        \n",
    "        # store training set info\n",
    "        self.train_items_set = set(train_df['movieId'].unique())\n",
    "        self.train_users_set = set(train_df['userId'].unique())\n",
    "        self.global_mean = train_df['rating'].mean()\n",
    "        self.user_means = train_df.groupby('userId')['rating'].mean().to_dict()\n",
    "        \n",
    "        # convert to surprise format\n",
    "        rating_min = train_df['rating'].min()\n",
    "        rating_max = train_df['rating'].max()\n",
    "        self.reader = Reader(rating_scale=(rating_min, rating_max))\n",
    "        \n",
    "        data = Dataset.load_from_df(\n",
    "            train_df[['userId', 'movieId', 'rating']], \n",
    "            self.reader\n",
    "        )\n",
    "        trainset = data.build_full_trainset()\n",
    "        \n",
    "        # train svd\n",
    "        self.model = SVD(\n",
    "            n_factors=self.n_factors,\n",
    "            n_epochs=self.n_epochs,\n",
    "            lr_all=self.lr_all,\n",
    "            reg_all=self.reg_all,\n",
    "            random_state=self.random_state,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        self.model.fit(trainset)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"cf model fitted - items: {len(self.train_items_set):,}, users: {len(self.train_users_set):,}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, pairs_df: pd.DataFrame) -> np.ndarray:\n",
    "        # predict ratings using svd\n",
    "        predictions = []\n",
    "        for uid, mid in zip(pairs_df['userId'].values, pairs_df['movieId'].values):\n",
    "            pred = self.model.predict(uid, mid)\n",
    "            predictions.append(pred.est)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_fallback(self, pairs_df: pd.DataFrame) -> np.ndarray:\n",
    "        # fallback: use user mean or global mean for cold items\n",
    "        predictions = []\n",
    "        for uid in pairs_df['userId'].values:\n",
    "            if uid in self.user_means:\n",
    "                predictions.append(self.user_means[uid])\n",
    "            else:\n",
    "                predictions.append(self.global_mean)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def save(self, path: Path):\n",
    "        \"\"\"Save model to disk.\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'model': self.model,\n",
    "                'train_items_set': self.train_items_set,\n",
    "                'train_users_set': self.train_users_set,\n",
    "                'global_mean': self.global_mean,\n",
    "                'user_means': self.user_means,\n",
    "                'reader': self.reader,\n",
    "                'n_factors': self.n_factors,\n",
    "                'n_epochs': self.n_epochs\n",
    "            }, f)\n",
    "        print(f\"CF model saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: Path):\n",
    "        \"\"\"Load model from disk.\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        model = cls(n_factors=data['n_factors'], n_epochs=data['n_epochs'])\n",
    "        model.model = data['model']\n",
    "        model.train_items_set = data['train_items_set']\n",
    "        model.train_users_set = data['train_users_set']\n",
    "        model.global_mean = data['global_mean']\n",
    "        model.user_means = data['user_means']\n",
    "        model.reader = data['reader']\n",
    "        print(f\"CF model loaded from {path}\")\n",
    "        return model\n",
    "\n",
    "\n",
    "print(\"cfmodel defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Content-Based Filtering (CBF) - Semantic Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2.1 EMBEDDING COMPUTATION & CACHING\n",
    "\n",
    "def compute_or_load_embeddings(movies_df: pd.DataFrame, model_name: str, cache_dir: Path) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute or load cached embeddings for movie text.\n",
    "    \n",
    "    Args:\n",
    "        movies_df: DataFrame with columns [movieId, text]\n",
    "        model_name: Name of the sentence-transformer model\n",
    "        cache_dir: Directory for caching embeddings\n",
    "        \n",
    "    Returns:\n",
    "        embeddings: numpy array of shape (n_movies, embedding_dim)\n",
    "        movieId_to_idx: dict mapping movieId -> embedding row index\n",
    "        idx_to_movieId: dict mapping embedding row index -> movieId\n",
    "    \"\"\"\n",
    "    # create safe filename for cache\n",
    "    safe_name = model_name.replace('/', '_')\n",
    "    emb_path = cache_dir / f'{safe_name}_emb.npy'\n",
    "    mapping_path = cache_dir / f'{safe_name}_mapping.pkl'\n",
    "    \n",
    "    if emb_path.exists() and mapping_path.exists():\n",
    "        print(f\"loading cached embeddings for {model_name}...\")\n",
    "        embeddings = np.load(emb_path)\n",
    "        with open(mapping_path, 'rb') as f:\n",
    "            mappings = pickle.load(f)\n",
    "        movieId_to_idx = mappings['movieId_to_idx']\n",
    "        idx_to_movieId = mappings['idx_to_movieId']\n",
    "        print(f\"  Loaded embeddings: {embeddings.shape}\")\n",
    "        return embeddings, movieId_to_idx, idx_to_movieId\n",
    "    \n",
    "    print(f\"Computing embeddings for {model_name}...\")\n",
    "    \n",
    "    # load sentence transformer model\n",
    "    st_model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # sort movies by movieId for consistent ordering\n",
    "    movies_sorted = movies_df.sort_values('movieId').reset_index(drop=True)\n",
    "    \n",
    "    # create mappings\n",
    "    movieId_to_idx = {mid: idx for idx, mid in enumerate(movies_sorted['movieId'].values)}\n",
    "    idx_to_movieId = {idx: mid for mid, idx in movieId_to_idx.items()}\n",
    "    \n",
    "    # get texts\n",
    "    texts = movies_sorted['text'].fillna('').tolist()\n",
    "    \n",
    "    # compute embeddings in batches\n",
    "    print(f\"  Encoding {len(texts):,} movie texts...\")\n",
    "    embeddings = st_model.encode(\n",
    "        texts, \n",
    "        batch_size=128, \n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    # normalize embeddings for cosine similarity\n",
    "    embeddings = normalize(embeddings, axis=1)\n",
    "    \n",
    "    # cache\n",
    "    print(f\"  Caching embeddings to {emb_path}...\")\n",
    "    np.save(emb_path, embeddings)\n",
    "    with open(mapping_path, 'wb') as f:\n",
    "        pickle.dump({'movieId_to_idx': movieId_to_idx, 'idx_to_movieId': idx_to_movieId}, f)\n",
    "    \n",
    "    print(f\"  Embeddings computed and cached: {embeddings.shape}\")\n",
    "    return embeddings, movieId_to_idx, idx_to_movieId\n",
    "\n",
    "\n",
    "print(\"Embedding computation function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content-based filtering using semantic embeddings\n",
    "\n",
    "class CBFModel:\n",
    "    # ranking model: computes similarity between user profiles and items\n",
    "    # user profiles are weighted average of item embeddings\n",
    "    \n",
    "    def __init__(self, use_rating_weights=True):\n",
    "        self.use_rating_weights = use_rating_weights\n",
    "        self.user_profiles = None\n",
    "        self.item_embeddings = None\n",
    "        self.movieId_to_idx = None\n",
    "        self.idx_to_movieId = None\n",
    "        self.train_users = None\n",
    "        \n",
    "    def fit(self, train_df: pd.DataFrame, item_embeddings: np.ndarray, \n",
    "            movieId_to_idx: dict, verbose=True):\n",
    "        if verbose:\n",
    "            print(\"fitting cbf model\")\n",
    "        \n",
    "        self.item_embeddings = item_embeddings\n",
    "        self.movieId_to_idx = movieId_to_idx\n",
    "        self.idx_to_movieId = {v: k for k, v in movieId_to_idx.items()}\n",
    "        \n",
    "        # compute user means\n",
    "        self.global_mean = train_df['rating'].mean()\n",
    "        self.user_means = train_df.groupby('userId')['rating'].mean().to_dict()\n",
    "        self.train_users = set(train_df['userId'].unique())\n",
    "        \n",
    "        # build user profiles\n",
    "        if verbose:\n",
    "            print(\"  Building user profiles...\")\n",
    "        \n",
    "        user_items_ratings = train_df.groupby('userId').apply(\n",
    "            lambda x: list(zip(x['movieId'].values, x['rating'].values))\n",
    "        ).to_dict()\n",
    "        \n",
    "        self.user_profiles = {}\n",
    "        emb_dim = item_embeddings.shape[1]\n",
    "        \n",
    "        for uid, items_ratings in tqdm(user_items_ratings.items(), \n",
    "                                       disable=not verbose, \n",
    "                                       desc=\"  User profiles\"):\n",
    "            profile = np.zeros(emb_dim)\n",
    "            total_weight = 0\n",
    "            \n",
    "            user_mean = self.user_means.get(uid, self.global_mean)\n",
    "            \n",
    "            for mid, rating in items_ratings:\n",
    "                if mid not in self.movieId_to_idx:\n",
    "                    continue\n",
    "                    \n",
    "                idx = self.movieId_to_idx[mid]\n",
    "                \n",
    "                if self.use_rating_weights:\n",
    "                    # weight by deviation from user mean (directional preference)\n",
    "                    weight = rating - user_mean + 3.0  # Add offset to keep positive\n",
    "                else:\n",
    "                    weight = 1.0\n",
    "                \n",
    "                profile += weight * item_embeddings[idx]\n",
    "                total_weight += abs(weight)\n",
    "            \n",
    "            if total_weight > 0:\n",
    "                profile /= total_weight\n",
    "                # normalize for cosine similarity\n",
    "                norm = np.linalg.norm(profile)\n",
    "                if norm > 0:\n",
    "                    profile /= norm\n",
    "            \n",
    "            self.user_profiles[uid] = profile\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"cbf model fitted - {len(self.user_profiles):,} user profiles created\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, pairs_df: pd.DataFrame) -> np.ndarray:\n",
    "        # compute similarity scores (not ratings)\n",
    "        scores = []\n",
    "        \n",
    "        for uid, mid in zip(pairs_df['userId'].values, pairs_df['movieId'].values):\n",
    "            if uid not in self.user_profiles or mid not in self.movieId_to_idx:\n",
    "                scores.append(0.0)  # unseen users/items get 0 similarity\n",
    "                continue\n",
    "            \n",
    "            user_profile = self.user_profiles[uid]\n",
    "            item_emb = self.item_embeddings[self.movieId_to_idx[mid]]\n",
    "            \n",
    "            # cosine similarity (dot product of normalized vectors)\n",
    "            sim = np.dot(user_profile, item_emb)\n",
    "            scores.append(sim)\n",
    "        \n",
    "        return np.array(scores)\n",
    "    \n",
    "    def save(self, path: Path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'user_profiles': self.user_profiles,\n",
    "                'item_embeddings': self.item_embeddings,\n",
    "                'movieId_to_idx': self.movieId_to_idx,\n",
    "                'idx_to_movieId': self.idx_to_movieId,\n",
    "                'train_users': self.train_users,\n",
    "                'global_mean': self.global_mean,\n",
    "                'user_means': self.user_means,\n",
    "                'use_rating_weights': self.use_rating_weights\n",
    "            }, f)\n",
    "        print(f\"cbf model saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: Path):\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        model = cls(use_rating_weights=data['use_rating_weights'])\n",
    "        model.user_profiles = data['user_profiles']\n",
    "        model.item_embeddings = data['item_embeddings']\n",
    "        model.movieId_to_idx = data['movieId_to_idx']\n",
    "        model.idx_to_movieId = data['idx_to_movieId']\n",
    "        model.train_users = data['train_users']\n",
    "        model.global_mean = data['global_mean']\n",
    "        model.user_means = data['user_means']\n",
    "        \n",
    "        print(f\"cbf model loaded from {path} ({len(model.user_profiles):,} profiles)\")\n",
    "        return model\n",
    "\n",
    "\n",
    "print(\"cbfmodel defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2.3 EMBEDDING MODEL COMPARISON\n",
    "\n",
    "def compare_embedding_models(train_df: pd.DataFrame, val_df: pd.DataFrame, \n",
    "                            movies_df: pd.DataFrame, model_names: list,\n",
    "                            cache_dir: Path, n_sample: int = 10000):\n",
    "    \"\"\"\n",
    "    Compare embedding models by evaluating CBF performance on validation set.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training ratings\n",
    "        val_df: Validation ratings (subset of test for quick eval)\n",
    "        movies_df: Movies with text\n",
    "        model_names: List of embedding model names to compare\n",
    "        cache_dir: Directory for caching\n",
    "        n_sample: Number of validation samples to use\n",
    "        \n",
    "    Returns:\n",
    "        results_df: DataFrame with MAE/RMSE for each model\n",
    "        best_model_name: Name of best performing model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EMBEDDING MODEL COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # sample validation set\n",
    "    if len(val_df) > n_sample:\n",
    "        val_sample = val_df.sample(n=n_sample, random_state=SEED)\n",
    "    else:\n",
    "        val_sample = val_df\n",
    "    \n",
    "    print(f\"Validation sample size: {len(val_sample):,}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f\"\\n--- {model_name}\")\n",
    "        \n",
    "        # load or compute embeddings\n",
    "        embeddings, movieId_to_idx, _ = compute_or_load_embeddings(\n",
    "            movies_df, model_name, cache_dir\n",
    "        )\n",
    "        \n",
    "        # train CBF model\n",
    "        cbf = CBFModel(use_rating_weights=True)\n",
    "        cbf.fit(train_df, embeddings, movieId_to_idx, verbose=False)\n",
    "        \n",
    "        # predict on validation\n",
    "        preds = cbf.predict(val_sample)\n",
    "        true_ratings = val_sample['rating'].values\n",
    "        \n",
    "        # compute metrics\n",
    "        mae = np.mean(np.abs(preds - true_ratings))\n",
    "        rmse = np.sqrt(np.mean((preds - true_ratings) ** 2))\n",
    "        \n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'embedding_dim': embeddings.shape[1]\n",
    "        })\n",
    "        \n",
    "        print(f\"  MAE: {mae:.4f}, RMSE: {rmse:.4f}, Dim: {embeddings.shape[1]}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # find best model (by MAE)\n",
    "    best_idx = results_df['MAE'].idxmin()\n",
    "    best_model_name = results_df.loc[best_idx, 'model']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BEST MODEL: {best_model_name} (MAE: {results_df.loc[best_idx, 'MAE']:.4f})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return results_df, best_model_name\n",
    "\n",
    "\n",
    "print(\"Embedding comparison function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Hybrid Model (switching: CF for warm items, CBF fallback for cold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid model: switching strategy for cold-start items\n",
    "\n",
    "class HybridModel:\n",
    "    # uses cf for warm items, cbf for cold items\n",
    "    # no blending since cf returns ratings and cbf returns similarity\n",
    "    \n",
    "    def __init__(self, cf_model: CFModel, cbf_model: CBFModel):\n",
    "        self.cf_model = cf_model\n",
    "        self.cbf_model = cbf_model\n",
    "        \n",
    "    def predict(self, pairs_df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict scores using switching strategy.\n",
    "        \n",
    "        Returns:\n",
    "        - For warm items: CF ratings (0.5-5.0) - proven best for regression\n",
    "        - For cold items: CBF similarity (0-1) - only model that works\n",
    "        \n",
    "        Note: Returns mixed units, but this is appropriate since:\n",
    "        - Warm items: CF has learned item factors → use ratings\n",
    "        - Cold items: CF cannot predict → must use CBF similarity\n",
    "        \n",
    "        Args:\n",
    "            pairs_df: DataFrame with columns [userId, movieId]\n",
    "            \n",
    "        Returns:\n",
    "            Array of predictions (mixed units: ratings for warm, similarity for cold)\n",
    "        \"\"\"\n",
    "        # get predictions from both models\n",
    "        cf_preds = self.cf_model.predict(pairs_df)  # Ratings (0.5-5.0)\n",
    "        cbf_preds = self.cbf_model.predict(pairs_df)  # Similarity (0-1)\n",
    "        \n",
    "        # determine which items are warm (in CF training set)\n",
    "        warm_mask = pairs_df['movieId'].isin(self.cf_model.train_items_set).values\n",
    "        \n",
    "        # switching: Switch between CF and CBF based on item warmth\n",
    "        # NO blending - use the right model for each item\n",
    "        predictions = np.where(\n",
    "            warm_mask,\n",
    "            cf_preds,   # Warm: CF predictions (ratings)\n",
    "            cbf_preds   # Cold: CBF predictions (similarity)\n",
    "        )\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_for_cold_regime(self, pairs_df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        For cold regime evaluation: use CBF for cold items, CF fallback otherwise.\n",
    "        This is specific to the cold-start evaluation where all items are cold.\n",
    "        \"\"\"\n",
    "        return self.cbf_model.predict(pairs_df)\n",
    "\n",
    "\n",
    "def tune_hybrid_alpha(cf_model: CFModel, cbf_model: CBFModel, \n",
    "                      val_df: pd.DataFrame, alpha_values: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Tune hybrid alpha parameter on validation set.\n",
    "    \n",
    "    Args:\n",
    "        cf_model: Trained CF model\n",
    "        cbf_model: Trained CBF model\n",
    "        val_df: Validation DataFrame\n",
    "        alpha_values: List of alpha values to try\n",
    "        \n",
    "    Returns:\n",
    "        best_alpha: Best alpha value\n",
    "        results_df: DataFrame with metrics for each alpha\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"HYBRID ALPHA TUNING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Validation size: {len(val_df):,}\")\n",
    "    \n",
    "    results = []\n",
    "    true_ratings = val_df['rating'].values\n",
    "    \n",
    "    # pre-compute base predictions\n",
    "    cf_preds = cf_model.predict(val_df)\n",
    "    cbf_preds = cbf_model.predict(val_df)\n",
    "    warm_mask = val_df['movieId'].isin(cf_model.train_items_set).values\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        # blend predictions\n",
    "        hybrid_preds = np.where(\n",
    "            warm_mask,\n",
    "            alpha * cf_preds + (1 - alpha) * cbf_preds,\n",
    "            cbf_preds\n",
    "        )\n",
    "        \n",
    "        mae = np.mean(np.abs(hybrid_preds - true_ratings))\n",
    "        rmse = np.sqrt(np.mean((hybrid_preds - true_ratings) ** 2))\n",
    "        \n",
    "        results.append({\n",
    "            'alpha': alpha,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse\n",
    "        })\n",
    "        \n",
    "        print(f\"  alpha={alpha:.2f}: MAE={mae:.4f}, RMSE={rmse:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # find best alpha\n",
    "    best_idx = results_df['MAE'].idxmin()\n",
    "    best_alpha = results_df.loc[best_idx, 'alpha']\n",
    "    \n",
    "    print(f\"\\nBest alpha: {best_alpha} (MAE: {results_df.loc[best_idx, 'MAE']:.4f})\")\n",
    "    \n",
    "    return best_alpha, results_df\n",
    "\n",
    "\n",
    "print(\"HybridModel class and alpha tuning function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 REGRESSION METRICS\n",
    "\n",
    "def compute_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Compute regression metrics: MAE and RMSE.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True ratings\n",
    "        y_pred: Predicted ratings\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with MAE and RMSE\n",
    "    \"\"\"\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    return {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "\n",
    "def evaluate_regression(predict_fn, test_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate model on regression metrics.\n",
    "    \n",
    "    Args:\n",
    "        predict_fn: Function that takes pairs_df and returns predictions\n",
    "        test_df: Test DataFrame with [userId, movieId, rating]\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metrics\n",
    "    \"\"\"\n",
    "    predictions = predict_fn(test_df)\n",
    "    true_ratings = test_df['rating'].values\n",
    "    return compute_regression_metrics(true_ratings, predictions)\n",
    "\n",
    "\n",
    "print(\"Regression metrics defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking metrics with negative sampling\n",
    "\n",
    "def precision_at_k(ranked_items: list, positives: set, k: int) -> float:\n",
    "    # proportion of top-k that are relevant\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    top_k = set(ranked_items[:k])\n",
    "    return len(top_k.intersection(positives)) / k\n",
    "\n",
    "\n",
    "def recall_at_k(ranked_items: list, positives: set, k: int) -> float:\n",
    "    # proportion of positives found in top-k\n",
    "    if len(positives) == 0:\n",
    "        return 0.0\n",
    "    top_k = set(ranked_items[:k])\n",
    "    return len(top_k.intersection(positives)) / len(positives)\n",
    "\n",
    "\n",
    "def dcg_at_k(ranked_items: list, positives: set, k: int) -> float:\n",
    "    # discounted cumulative gain (binary relevance)\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(ranked_items[:k]):\n",
    "        if item in positives:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(ranked_items: list, positives: set, k: int) -> float:\n",
    "    # normalized dcg\n",
    "    dcg = dcg_at_k(ranked_items, positives, k)\n",
    "    \n",
    "    # ideal dcg: all positives at the top\n",
    "    ideal_n = min(len(positives), k)\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(ideal_n))\n",
    "    \n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def evaluate_ranking(predict_fn, train_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                     candidate_pool: set, K: int = 10, N_neg: int = 100,\n",
    "                     max_users: int = 5000, seed: int = SEED, \n",
    "                     relevance_threshold: float = 4.0) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate ranking metrics with negative sampling.\n",
    "    \n",
    "    Protocol per user:\n",
    "    1. positives = items in test rated >= relevance_threshold\n",
    "    2. forbidden = train items + ALL test items (avoid false negatives)\n",
    "    3. negatives = sample N_neg from candidate_pool excluding forbidden\n",
    "    4. candidates = positives + negatives (shuffled to avoid ordering bias)\n",
    "    5. Score all candidates, rank by score (with tie-breaking noise)\n",
    "    6. Compute Recall@K, NDCG@K, Precision@K\n",
    "    \n",
    "    Note: Candidates are shuffled and tiny noise is added to scores to ensure\n",
    "    fair ranking when scores are equal (e.g., CF_fallback gives same score to all items)\n",
    "    \n",
    "    Args:\n",
    "        predict_fn: Function that takes pairs_df and returns scores\n",
    "        train_df: Training DataFrame\n",
    "        test_df: Test DataFrame\n",
    "        candidate_pool: Set of item IDs to sample negatives from\n",
    "        K: Top-K for metrics\n",
    "        N_neg: Number of negatives to sample per user\n",
    "        max_users: Maximum users to evaluate (for efficiency)\n",
    "        seed: Random seed\n",
    "        relevance_threshold: Minimum rating to consider an item \"relevant\" (default 4.0)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with average metrics\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # build user -> train items mapping\n",
    "    train_user_items = build_user_item_map(train_df)\n",
    "    \n",
    "    # build user -> ALL test items (for forbidden set to avoid false negatives)\n",
    "    all_test_user_items = build_user_item_map(test_df)\n",
    "    \n",
    "    # build user -> RELEVANT test items (rating >= threshold) for metrics\n",
    "    test_relevant = test_df[test_df['rating'] >= relevance_threshold]\n",
    "    relevant_user_items = build_user_item_map(test_relevant)\n",
    "    \n",
    "    print(f\"  Relevance threshold: rating >= {relevance_threshold}\")\n",
    "    print(f\"  Relevant test items: {len(test_relevant):,} / {len(test_df):,} ({100*len(test_relevant)/len(test_df):.1f}%)\")\n",
    "    \n",
    "    # get users who have at least one relevant item\n",
    "    test_users = list(relevant_user_items.keys())\n",
    "    \n",
    "    # sample users if too many\n",
    "    if len(test_users) > max_users:\n",
    "        print(f\"  Sampling {max_users:,} users from {len(test_users):,} for ranking eval\")\n",
    "        test_users = list(rng.choice(test_users, size=max_users, replace=False))\n",
    "    \n",
    "    # metrics accumulators\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_ndcg = []\n",
    "    \n",
    "    for uid in tqdm(test_users, desc=\"  Ranking eval\"):\n",
    "        # positives = items in test rated >= threshold (for metrics)\n",
    "        positives = relevant_user_items.get(uid, set())\n",
    "        if len(positives) == 0:\n",
    "            continue\n",
    "        \n",
    "        # forbidden items: train + ALL test items (not just relevant ones)\n",
    "        # this prevents sampling any test item as a negative\n",
    "        train_items = train_user_items.get(uid, set())\n",
    "        all_test_items = all_test_user_items.get(uid, set())\n",
    "        forbidden = train_items.union(all_test_items)\n",
    "        \n",
    "        # sample negatives\n",
    "        negatives = sample_negatives(candidate_pool, forbidden, N_neg, rng)\n",
    "        if len(negatives) == 0:\n",
    "            continue\n",
    "        \n",
    "        # candidates = positives + negatives\n",
    "        # IMPORTANT: Shuffle candidates to avoid ordering bias when scores are equal\n",
    "        # (e.g., CF_fallback gives same score to all items, so original order would bias results)\n",
    "        candidates = list(positives) + negatives\n",
    "        shuffle_indices = rng.permutation(len(candidates))\n",
    "        candidates_shuffled = [candidates[i] for i in shuffle_indices]\n",
    "        \n",
    "        # create pairs DataFrame for scoring\n",
    "        pairs_df = pd.DataFrame({\n",
    "            'userId': [uid] * len(candidates_shuffled),\n",
    "            'movieId': candidates_shuffled\n",
    "        })\n",
    "        \n",
    "        # get scores\n",
    "        scores = predict_fn(pairs_df)\n",
    "        \n",
    "        # rank by score (descending) with random tie-breaking\n",
    "        # add tiny random noise to break ties fairly when scores are equal\n",
    "        noise = rng.uniform(0, 1e-10, size=len(scores))\n",
    "        scores_with_noise = scores + noise\n",
    "        ranked_indices = np.argsort(-scores_with_noise)\n",
    "        ranked_items = [candidates_shuffled[i] for i in ranked_indices]\n",
    "        \n",
    "        # compute metrics\n",
    "        all_precision.append(precision_at_k(ranked_items, positives, K))\n",
    "        all_recall.append(recall_at_k(ranked_items, positives, K))\n",
    "        all_ndcg.append(ndcg_at_k(ranked_items, positives, K))\n",
    "    \n",
    "    n_evaluated = len(all_precision)\n",
    "    \n",
    "    return {\n",
    "        f'Precision@{K}': np.mean(all_precision) if all_precision else 0.0,\n",
    "        f'Recall@{K}': np.mean(all_recall) if all_recall else 0.0,\n",
    "        f'NDCG@{K}': np.mean(all_ndcg) if all_ndcg else 0.0,\n",
    "        'n_users_evaluated': n_evaluated\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Ranking metrics with negative sampling defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Experiments and Save Results\n",
    "\n",
    "### 7.1 Warm Regime Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1.1 LOAD EMBEDDINGS (all-MiniLM-L6-v2)\n",
    "\n",
    "print()\n",
    "print(\"STEP 1: LOAD EMBEDDINGS\")\n",
    "print()\n",
    "\n",
    "# use all-MiniLM-L6-v2 embeddings (already computed/cached)\n",
    "best_embedding_model = 'all-MiniLM-L6-v2'\n",
    "print(f\"Using embedding model: {best_embedding_model}\")\n",
    "\n",
    "# load embeddings\n",
    "best_embeddings, best_movieId_to_idx, best_idx_to_movieId = compute_or_load_embeddings(\n",
    "    movies, best_embedding_model, CACHE_DIR\n",
    ")\n",
    "\n",
    "print(f\"Embeddings loaded: {best_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1.2 TRAIN CF MODEL (WARM REGIME)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: TRAIN CF MODEL (Surprise SVD)\")\n",
    "print()\n",
    "\n",
    "# check for cached model\n",
    "cf_warm_cache_path = CACHE_DIR / 'cf_model_warm.pkl'\n",
    "\n",
    "if cf_warm_cache_path.exists():\n",
    "    print(\"loading cached CF model...\")\n",
    "    cf_warm = CFModel.load(cf_warm_cache_path)\n",
    "else:\n",
    "    cf_warm = CFModel(n_factors=100, n_epochs=20, random_state=SEED)\n",
    "    cf_warm.fit(train_warm, verbose=True)\n",
    "    cf_warm.save(cf_warm_cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1.3 TRAIN CBF MODEL WITH BEST EMBEDDING (WARM REGIME)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"STEP 3: TRAIN CBF MODEL with {best_embedding_model}\")\n",
    "print()\n",
    "\n",
    "# load best embeddings\n",
    "best_embeddings, best_movieId_to_idx, best_idx_to_movieId = compute_or_load_embeddings(\n",
    "    movies, best_embedding_model, CACHE_DIR\n",
    ")\n",
    "\n",
    "# train or load CBF model with caching\n",
    "cbf_warm_cache_path = CACHE_DIR / f\"cbf_warm_{best_embedding_model.replace('/', '_')}.pkl\"\n",
    "\n",
    "if cbf_warm_cache_path.exists():\n",
    "    print(f\"loading cached CBF model from {cbf_warm_cache_path}...\")\n",
    "    cbf_warm = CBFModel.load(cbf_warm_cache_path)\n",
    "else:\n",
    "    print(\"training new CBF model...\")\n",
    "    cbf_warm = CBFModel(use_rating_weights=True)\n",
    "    cbf_warm.fit(train_warm, best_embeddings, best_movieId_to_idx, verbose=True)\n",
    "    cbf_warm.save(cbf_warm_cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1.4 CREATE switching HYBRID MODEL\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: CREATE switching HYBRID MODEL\")\n",
    "print()\n",
    "\n",
    "# note: No alpha tuning needed for switching hybrid!\n",
    "# the hybrid uses a simple gating strategy:\n",
    "# - Warm items → CF (only model that works)\n",
    "# - Cold items → CBF (only model that works)\n",
    "# no blending of incompatible units (ratings vs similarity)\n",
    "\n",
    "print(\"creating switching hybrid model (CF for warm items, CBF for cold items)...\")\n",
    "hybrid_warm = HybridModel(cf_warm, cbf_warm)\n",
    "print(\"switching hybrid model created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1.5 FULL WARM REGIME EVALUATION\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: FULL WARM REGIME EVALUATION\")\n",
    "print()\n",
    "\n",
    "# WHAT WE'RE TESTING: Standard recommendation (both users and items are warm)\n",
    "# - All users have training history\n",
    "# - All test items exist in training set (CF has learned factors)\n",
    "# - This is the \"easiest\" regime - CF should perform best\n",
    "# - CBF: Ranks by similarity (no rating prediction, so MAE/RMSE = NaN)\n",
    "# - Hybrid: Uses CF for all items (since all are warm), should match CF\n",
    "\n",
    "# candidate pool for warm regime: items in training set\n",
    "warm_candidate_pool = set(train_warm['movieId'].unique())\n",
    "print(f\"warm candidate pool size: {len(warm_candidate_pool):,}\")\n",
    "\n",
    "warm_results = []\n",
    "\n",
    "# CF: Rating prediction model\n",
    "print(f\"\\n--- Evaluating CF\")\n",
    "reg_metrics_cf = evaluate_regression(cf_warm.predict, test_warm)\n",
    "print(f\"  Regression: MAE={reg_metrics_cf['MAE']:.4f}, RMSE={reg_metrics_cf['RMSE']:.4f}\")\n",
    "\n",
    "print(f\"  Computing ranking metrics...\")\n",
    "rank_metrics_cf = evaluate_ranking(\n",
    "    cf_warm.predict, train_warm, test_warm,\n",
    "    warm_candidate_pool, K=K, N_neg=N_NEG, max_users=MAX_USERS_RANK,\n",
    "    relevance_threshold=RELEVANCE_THRESHOLD\n",
    ")\n",
    "print(f\"  Ranking: Precision@{K}={rank_metrics_cf[f'Precision@{K}']:.4f}, \"\n",
    "      f\"Recall@{K}={rank_metrics_cf[f'Recall@{K}']:.4f}, \"\n",
    "      f\"NDCG@{K}={rank_metrics_cf[f'NDCG@{K}']:.4f}\")\n",
    "\n",
    "warm_results.append({\n",
    "    'Model': 'CF',\n",
    "    'Regime': 'Warm',\n",
    "    **reg_metrics_cf,\n",
    "    **{k: v for k, v in rank_metrics_cf.items() if k != 'n_users_evaluated'}\n",
    "})\n",
    "\n",
    "# CBF: Ranking-only model (similarity scores)\n",
    "print(f\"\\n--- Evaluating CBF\")\n",
    "print(f\"  (Ranking-only: using similarity scores)\")\n",
    "\n",
    "print(f\"  Computing ranking metrics...\")\n",
    "rank_metrics_cbf = evaluate_ranking(\n",
    "    cbf_warm.predict, train_warm, test_warm,\n",
    "    warm_candidate_pool, K=K, N_neg=N_NEG, max_users=MAX_USERS_RANK,\n",
    "    relevance_threshold=RELEVANCE_THRESHOLD\n",
    ")\n",
    "print(f\"  Ranking: Precision@{K}={rank_metrics_cbf[f'Precision@{K}']:.4f}, \"\n",
    "      f\"Recall@{K}={rank_metrics_cbf[f'Recall@{K}']:.4f}, \"\n",
    "      f\"NDCG@{K}={rank_metrics_cbf[f'NDCG@{K}']:.4f}\")\n",
    "print(f\"  Regression: N/A (CBF is a ranking model, not a rating predictor)\")\n",
    "\n",
    "warm_results.append({\n",
    "    'Model': 'CBF',\n",
    "    'Regime': 'Warm',\n",
    "    'MAE': float('nan'),\n",
    "    'RMSE': float('nan'),\n",
    "    **{k: v for k, v in rank_metrics_cbf.items() if k != 'n_users_evaluated'}\n",
    "})\n",
    "\n",
    "# hybrid: Blends CF and CBF\n",
    "print(f\"\\n--- Evaluating Hybrid\")\n",
    "reg_metrics_hybrid = evaluate_regression(hybrid_warm.predict, test_warm)\n",
    "print(f\"  Regression: MAE={reg_metrics_hybrid['MAE']:.4f}, RMSE={reg_metrics_hybrid['RMSE']:.4f}\")\n",
    "\n",
    "print(f\"  Computing ranking metrics...\")\n",
    "rank_metrics_hybrid = evaluate_ranking(\n",
    "    hybrid_warm.predict, train_warm, test_warm,\n",
    "    warm_candidate_pool, K=K, N_neg=N_NEG, max_users=MAX_USERS_RANK,\n",
    "    relevance_threshold=RELEVANCE_THRESHOLD\n",
    ")\n",
    "print(f\"  Ranking: Precision@{K}={rank_metrics_hybrid[f'Precision@{K}']:.4f}, \"\n",
    "      f\"Recall@{K}={rank_metrics_hybrid[f'Recall@{K}']:.4f}, \"\n",
    "      f\"NDCG@{K}={rank_metrics_hybrid[f'NDCG@{K}']:.4f}\")\n",
    "\n",
    "warm_results.append({\n",
    "    'Model': 'Hybrid',\n",
    "    'Regime': 'Warm',\n",
    "    **reg_metrics_hybrid,\n",
    "    **{k: v for k, v in rank_metrics_hybrid.items() if k != 'n_users_evaluated'}\n",
    "})\n",
    "\n",
    "# create results DataFrame\n",
    "warm_results_df = pd.DataFrame(warm_results)\n",
    "warm_results_df.to_csv(TABLES_DIR / 'warm_metrics.csv', index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"WARM REGIME RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(warm_results_df.to_string(index=False))\n",
    "print(f\"\\nResults saved to {TABLES_DIR / 'warm_metrics.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Cold Item Regime Experiments\n",
    "\n",
    "**Note:** For cold items, CF item-specific predictions are undefined. We report CF_fallback (user mean / global mean) as a baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cold item regime experiments\n",
    "\n",
    "print(\"\\ncold item regime experiments\")\n",
    "\n",
    "# filter cold items to only those with embeddings\n",
    "print(\"\\nfiltering cold items to those with embeddings\")\n",
    "cold_items_with_embeddings = cold_items.intersection(set(best_movieId_to_idx.keys()))\n",
    "missing_embeddings = cold_items - cold_items_with_embeddings\n",
    "\n",
    "print(f\"original cold items: {len(cold_items):,}\")\n",
    "print(f\"cold items with embeddings: {len(cold_items_with_embeddings):,}\")\n",
    "print(f\"missing embeddings: {len(missing_embeddings):,}\")\n",
    "\n",
    "if len(missing_embeddings) > 0:\n",
    "    print(f\"\\nwarning: {len(missing_embeddings)} cold items missing embeddings\")\n",
    "    print(f\"filtering test_cold to ensure cbf can make predictions\")\n",
    "    \n",
    "    # filter test_cold to only include items with embeddings\n",
    "    test_cold = test_cold[test_cold['movieId'].isin(cold_items_with_embeddings)].copy()\n",
    "    cold_items = cold_items_with_embeddings\n",
    "    \n",
    "    print(f\"Filtered test_cold: {len(test_cold):,} ratings remaining\")\n",
    "\n",
    "print(\"\\ntraining models on train_cold\")\n",
    "\n",
    "# train cf on train_cold\n",
    "cf_cold_cache_path = CACHE_DIR / 'cf_model_cold.pkl'\n",
    "\n",
    "if cf_cold_cache_path.exists():\n",
    "    print(\"loading cached cf model (cold)\")\n",
    "    cf_cold = CFModel.load(cf_cold_cache_path)\n",
    "else:\n",
    "    cf_cold = CFModel(n_factors=100, n_epochs=20, random_state=SEED)\n",
    "    cf_cold.fit(train_cold, verbose=True)\n",
    "    cf_cold.save(cf_cold_cache_path)\n",
    "\n",
    "# train or load cbf on train_cold\n",
    "cbf_cold_cache_path = CACHE_DIR / f\"cbf_cold_{best_embedding_model.replace('/', '_')}.pkl\"\n",
    "\n",
    "if cbf_cold_cache_path.exists():\n",
    "    print(f\"loading cached cbf model from {cbf_cold_cache_path}\")\n",
    "    cbf_cold = CBFModel.load(cbf_cold_cache_path)\n",
    "else:\n",
    "    print(\"training new cbf model for cold regime\")\n",
    "    cbf_cold = CBFModel(use_rating_weights=True)\n",
    "    cbf_cold.fit(train_cold, best_embeddings, best_movieId_to_idx, verbose=True)\n",
    "    cbf_cold.save(cbf_cold_cache_path)\n",
    "\n",
    "# create switching hybrid for cold regime\n",
    "hybrid_cold = HybridModel(cf_cold, cbf_cold)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"IMPORTANT NOTE ON COLD ITEM EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"CF item-specific predictions are UNDEFINED for cold items.\")\n",
    "print(\"We report CF_fallback (user mean / global mean) as a baseline.\")\n",
    "print(\"For the Hybrid model on cold items, only CBF predictions are used (switching).\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cold item evaluation\n",
    "\n",
    "print(\"\\nevaluating on test_cold (cold items only)\")\n",
    "\n",
    "# candidate pool: cold items only\n",
    "cold_candidate_pool = cold_items\n",
    "print(f\"cold candidate pool size: {len(cold_candidate_pool):,}\")\n",
    "print(f\"test cold size: {len(test_cold):,}\")\n",
    "\n",
    "cold_results = []\n",
    "\n",
    "# evaluate models on cold split\n",
    "\n",
    "print(\"\\nevaluating models on cold items\")\n",
    "\n",
    "# cbf is a ranking model for cold-start\n",
    "# uses cosine similarity scores (not ratings)\n",
    "# evaluate only on ranking metrics (precision@k, recall@k, ndcg)\n",
    "# regression metrics (mae/rmse) not applicable for cbf on cold-start\n",
    "\n",
    "# cf_fallback: uses user mean for all items\n",
    "print(f\"\\nevaluating cf_fallback on cold items\")\n",
    "print(\"  (baseline: user mean or global mean)\")\n",
    "\n",
    "reg_metrics_cf = evaluate_regression(cf_cold.predict_fallback, test_cold)\n",
    "print(f\"  Regression: MAE={reg_metrics_cf['MAE']:.4f}, RMSE={reg_metrics_cf['RMSE']:.4f}\")\n",
    "\n",
    "print(f\"  Computing ranking metrics...\")\n",
    "rank_metrics_cf = evaluate_ranking(\n",
    "    cf_cold.predict_fallback, train_cold, test_cold,\n",
    "    cold_candidate_pool, K=K, N_neg=N_NEG, max_users=MAX_USERS_RANK,\n",
    "    relevance_threshold=RELEVANCE_THRESHOLD\n",
    ")\n",
    "print(f\"  Ranking: Precision@{K}={rank_metrics_cf[f'Precision@{K}']:.4f}, \"\n",
    "      f\"Recall@{K}={rank_metrics_cf[f'Recall@{K}']:.4f}, \"\n",
    "      f\"NDCG@{K}={rank_metrics_cf[f'NDCG@{K}']:.4f}\")\n",
    "\n",
    "cold_results.append({\n",
    "    'Model': 'CF_fallback',\n",
    "    'Regime': 'Cold-Item',\n",
    "    **reg_metrics_cf,\n",
    "    **{k: v for k, v in rank_metrics_cf.items() if k != 'n_users_evaluated'}\n",
    "})\n",
    "\n",
    "# CBF: Ranking-only model (similarity scores)\n",
    "print(f\"\\n--- Evaluating CBF on cold items\")\n",
    "print(\"  (Ranking-only: using similarity scores)\")\n",
    "\n",
    "print(f\"  Computing ranking metrics...\")\n",
    "rank_metrics_cbf = evaluate_ranking(\n",
    "    cbf_cold.predict, train_cold, test_cold,\n",
    "    cold_candidate_pool, K=K, N_neg=N_NEG, max_users=MAX_USERS_RANK,\n",
    "    relevance_threshold=RELEVANCE_THRESHOLD\n",
    ")\n",
    "print(f\"  Ranking: Precision@{K}={rank_metrics_cbf[f'Precision@{K}']:.4f}, \"\n",
    "      f\"Recall@{K}={rank_metrics_cbf[f'Recall@{K}']:.4f}, \"\n",
    "      f\"NDCG@{K}={rank_metrics_cbf[f'NDCG@{K}']:.4f}\")\n",
    "print(f\"  Regression: N/A (CBF is a ranking model, not a rating predictor)\")\n",
    "\n",
    "cold_results.append({\n",
    "    'Model': 'CBF',\n",
    "    'Regime': 'Cold-Item',\n",
    "    'MAE': float('nan'),\n",
    "    'RMSE': float('nan'),\n",
    "    **{k: v for k, v in rank_metrics_cbf.items() if k != 'n_users_evaluated'}\n",
    "})\n",
    "\n",
    "# Hybrid: Uses CBF (similarity scores) for cold items\n",
    "print(f\"\\n--- Evaluating Hybrid on cold items\")\n",
    "print(\"  (switching strategy: CBF similarity scores for cold items)\")\n",
    "\n",
    "print(f\"  Computing ranking metrics...\")\n",
    "rank_metrics_hybrid = evaluate_ranking(\n",
    "    cbf_cold.predict, train_cold, test_cold,\n",
    "    cold_candidate_pool, K=K, N_neg=N_NEG, max_users=MAX_USERS_RANK,\n",
    "    relevance_threshold=RELEVANCE_THRESHOLD\n",
    ")\n",
    "print(f\"  Ranking: Precision@{K}={rank_metrics_hybrid[f'Precision@{K}']:.4f}, \"\n",
    "      f\"Recall@{K}={rank_metrics_hybrid[f'Recall@{K}']:.4f}, \"\n",
    "      f\"NDCG@{K}={rank_metrics_hybrid[f'NDCG@{K}']:.4f}\")\n",
    "print(f\"  Regression: N/A (Hybrid uses CBF similarity for cold items)\")\n",
    "\n",
    "cold_results.append({\n",
    "    'Model': 'Hybrid',\n",
    "    'Regime': 'Cold-Item',\n",
    "    'MAE': float('nan'),\n",
    "    'RMSE': float('nan'),\n",
    "    **{k: v for k, v in rank_metrics_hybrid.items() if k != 'n_users_evaluated'}\n",
    "})\n",
    "\n",
    "# create results dataframe\n",
    "cold_results_df = pd.DataFrame(cold_results)\n",
    "cold_results_df.to_csv(TABLES_DIR / 'cold_item_metrics.csv', index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COLD ITEM REGIME RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(cold_results_df.to_string(index=False))\n",
    "print(f\"\\nResults saved to {TABLES_DIR / 'cold_item_metrics.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Few-shot User Regime Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot user experiments\n",
    "\n",
    "print(\"\\nfew-shot user regime experiments\")\n",
    "print(\"note: using consistent user set across all k values for fair comparison\")\n",
    "\n",
    "# use same users across all k values\n",
    "# identify users who qualify for all k values (>k_max ratings)\n",
    "k_max = max(fewshot_splits.keys())\n",
    "user_rating_counts = ratings.groupby('userId').size()\n",
    "common_users = set(user_rating_counts[user_rating_counts > k_max].index)\n",
    "\n",
    "print(f\"\\nusing users with >{k_max} ratings for all k values\")\n",
    "print(f\"common users across k=1,3,5: {len(common_users):,}\")\n",
    "\n",
    "# filter each split to only include common users\n",
    "fewshot_splits_filtered = {}\n",
    "for k, (train_k, test_k) in fewshot_splits.items():\n",
    "    train_k_filtered = train_k[train_k['userId'].isin(common_users)].copy()\n",
    "    test_k_filtered = test_k[test_k['userId'].isin(common_users)].copy()\n",
    "    fewshot_splits_filtered[k] = (train_k_filtered, test_k_filtered)\n",
    "    print(f\"k={k}: {len(train_k_filtered):,} train, {len(test_k_filtered):,} test \"\n",
    "          f\"({test_k_filtered['userId'].nunique():,} users)\")\n",
    "\n",
    "fewshot_results = []\n",
    "\n",
    "for k, (train_k, test_k) in fewshot_splits_filtered.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FEW-SHOT k={k}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train size: {len(train_k):,}, Test size: {len(test_k):,}\")\n",
    "    \n",
    "    # train CF model\n",
    "    cf_k_cache_path = CACHE_DIR / f'cf_model_fewshot_k{k}.pkl'\n",
    "    \n",
    "    if cf_k_cache_path.exists():\n",
    "        print(f\"loading cached CF model (k={k})...\")\n",
    "        cf_k = CFModel.load(cf_k_cache_path)\n",
    "    else:\n",
    "        cf_k = CFModel(n_factors=50, n_epochs=15, random_state=SEED)  # Smaller for few-shot\n",
    "        cf_k.fit(train_k, verbose=True)\n",
    "        cf_k.save(cf_k_cache_path)\n",
    "    \n",
    "    # train CBF model (or load cached user profiles if available)\n",
    "    cbf_k_cache_path = CACHE_DIR / f'cbf_profiles_fewshot_k{k}.pkl'\n",
    "    \n",
    "    if cbf_k_cache_path.exists():\n",
    "        print(f\"loading cached CBF model (k={k})...\")\n",
    "        cbf_k = CBFModel.load(cbf_k_cache_path)\n",
    "    else:\n",
    "        print(f\"training new CBF model (k={k})...\")\n",
    "        cbf_k = CBFModel(use_rating_weights=True)\n",
    "        cbf_k.fit(train_k, best_embeddings, best_movieId_to_idx, verbose=True)\n",
    "        cbf_k.save(cbf_k_cache_path)\n",
    "    \n",
    "    # create switching hybrid\n",
    "    hybrid_k = HybridModel(cf_k, cbf_k)\n",
    "    \n",
    "    # candidate pool: items in training set\n",
    "    fewshot_candidate_pool = set(train_k['movieId'].unique())\n",
    "    print(f\"Candidate pool size: {len(fewshot_candidate_pool):,}\")\n",
    "    \n",
    "    # sample test set for efficiency (100k ratings max)\n",
    "    # use consistent seed offset across all k values for fair comparison\n",
    "    if len(test_k) > 100000:\n",
    "        print(f\"  Sampling 100k test ratings from {len(test_k):,} for efficiency\")\n",
    "        test_k_sample = test_k.sample(n=100000, random_state=SEED + 100)  # Fixed seed\n",
    "    else:\n",
    "        test_k_sample = test_k\n",
    "    \n",
    "    # IMPORTANT: For regression evaluation, filter to only items in training set\n",
    "    # this ensures fair comparison between CF and Hybrid (both evaluate on same items)\n",
    "    # without this, Hybrid uses CBF similarity (0-1) for items not in train_k,\n",
    "    # which produces terrible MAE/RMSE when compared against true ratings (1-5)\n",
    "    test_k_regression = test_k_sample[test_k_sample['movieId'].isin(fewshot_candidate_pool)]\n",
    "    print(f\"  Regression test set: {len(test_k_regression):,} ratings (filtered to warm items only)\")\n",
    "    \n",
    "    # evaluate CF\n",
    "    print(f\"\\n--- Evaluating CF (k={k})\")\n",
    "    reg_metrics_cf = evaluate_regression(cf_k.predict, test_k_regression)\n",
    "    print(f\"  Regression: MAE={reg_metrics_cf['MAE']:.4f}, RMSE={reg_metrics_cf['RMSE']:.4f}\")\n",
    "    \n",
    "    print(f\"  Computing ranking metrics...\")\n",
    "    rank_metrics_cf = evaluate_ranking(\n",
    "        cf_k.predict, train_k, test_k_sample,\n",
    "        fewshot_candidate_pool, K=K, N_neg=N_NEG, max_users=2000,\n",
    "        relevance_threshold=RELEVANCE_THRESHOLD\n",
    "    )\n",
    "    print(f\"  Ranking: Precision@{K}={rank_metrics_cf[f'Precision@{K}']:.4f}, \"\n",
    "          f\"Recall@{K}={rank_metrics_cf[f'Recall@{K}']:.4f}, \"\n",
    "          f\"NDCG@{K}={rank_metrics_cf[f'NDCG@{K}']:.4f}\")\n",
    "    \n",
    "    fewshot_results.append({\n",
    "        'Model': 'CF',\n",
    "        'Regime': f'Few-shot k={k}',\n",
    "        'k': k,\n",
    "        **reg_metrics_cf,\n",
    "        **{key: val for key, val in rank_metrics_cf.items() if key != 'n_users_evaluated'}\n",
    "    })\n",
    "    \n",
    "    # evaluate CBF (ranking-only)\n",
    "    print(f\"\\n--- Evaluating CBF (k={k})\")\n",
    "    print(f\"  (Ranking-only: using similarity scores)\")\n",
    "    \n",
    "    print(f\"  Computing ranking metrics...\")\n",
    "    rank_metrics_cbf = evaluate_ranking(\n",
    "        cbf_k.predict, train_k, test_k_sample,\n",
    "        fewshot_candidate_pool, K=K, N_neg=N_NEG, max_users=2000,\n",
    "        relevance_threshold=RELEVANCE_THRESHOLD\n",
    "    )\n",
    "    print(f\"  Ranking: Precision@{K}={rank_metrics_cbf[f'Precision@{K}']:.4f}, \"\n",
    "          f\"Recall@{K}={rank_metrics_cbf[f'Recall@{K}']:.4f}, \"\n",
    "          f\"NDCG@{K}={rank_metrics_cbf[f'NDCG@{K}']:.4f}\")\n",
    "    print(f\"  Regression: N/A (CBF is a ranking model)\")\n",
    "    \n",
    "    fewshot_results.append({\n",
    "        'Model': 'CBF',\n",
    "        'Regime': f'Few-shot k={k}',\n",
    "        'k': k,\n",
    "        'MAE': float('nan'),\n",
    "        'RMSE': float('nan'),\n",
    "        **{key: val for key, val in rank_metrics_cbf.items() if key != 'n_users_evaluated'}\n",
    "    })\n",
    "    \n",
    "    # evaluate Hybrid\n",
    "    print(f\"\\n--- Evaluating Hybrid (k={k})\")\n",
    "    reg_metrics_hybrid = evaluate_regression(hybrid_k.predict, test_k_regression)\n",
    "    print(f\"  Regression: MAE={reg_metrics_hybrid['MAE']:.4f}, RMSE={reg_metrics_hybrid['RMSE']:.4f}\")\n",
    "    \n",
    "    print(f\"  Computing ranking metrics...\")\n",
    "    rank_metrics_hybrid = evaluate_ranking(\n",
    "        hybrid_k.predict, train_k, test_k_sample,\n",
    "        fewshot_candidate_pool, K=K, N_neg=N_NEG, max_users=2000,\n",
    "        relevance_threshold=RELEVANCE_THRESHOLD\n",
    "    )\n",
    "    print(f\"  Ranking: Precision@{K}={rank_metrics_hybrid[f'Precision@{K}']:.4f}, \"\n",
    "          f\"Recall@{K}={rank_metrics_hybrid[f'Recall@{K}']:.4f}, \"\n",
    "          f\"NDCG@{K}={rank_metrics_hybrid[f'NDCG@{K}']:.4f}\")\n",
    "    \n",
    "    fewshot_results.append({\n",
    "        'Model': 'Hybrid',\n",
    "        'Regime': f'Few-shot k={k}',\n",
    "        'k': k,\n",
    "        **reg_metrics_hybrid,\n",
    "        **{key: val for key, val in rank_metrics_hybrid.items() if key != 'n_users_evaluated'}\n",
    "    })\n",
    "\n",
    "# create results dataframe\n",
    "fewshot_results_df = pd.DataFrame(fewshot_results)\n",
    "fewshot_results_df.to_csv(TABLES_DIR / 'fewshot_metrics.csv', index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FEW-SHOT REGIME RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(fewshot_results_df.to_string(index=False))\n",
    "print(f\"\\nResults saved to {TABLES_DIR / 'fewshot_metrics.csv'}\")\n",
    "\n",
    "# EXPECTED BEHAVIOR (with consistent user set):\n",
    "# - k=1 → k=3 → k=5: Performance should INCREASE monotonically\n",
    "# - More training data (higher k) → better user profiles → better predictions\n",
    "# - If k=5 < k=3: likely due to overfitting or noise (check sample sizes)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INTERPRETATION (Consistent User Set):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"With same users across all k values:\")\n",
    "print(\"- k=1: Limited signal (1 rating/user) → Lower performance\")\n",
    "print(\"- k=3: More signal (3 ratings/user) → Better performance\")  \n",
    "print(\"- k=5: Most signal (5 ratings/user) → Best performance (expected)\")\n",
    "print(\"Hybrid ≈ CF because most test items are warm (in training set)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plots and Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 WARM PERFORMANCE VS ITEM POPULARITY\n",
    "\n",
    "print()\n",
    "print(\"PLOT 8.1: Warm Performance vs Item Popularity\")\n",
    "print()\n",
    "\n",
    "# define models dictionary for plotting\n",
    "models_warm = {\n",
    "    'CF': cf_warm.predict,\n",
    "    'CBF': cbf_warm.predict,\n",
    "    'Hybrid': hybrid_warm.predict\n",
    "}\n",
    "\n",
    "# compute item popularity in training set\n",
    "item_popularity = train_warm.groupby('movieId').size().to_dict()\n",
    "\n",
    "# define popularity buckets\n",
    "def get_popularity_bucket(pop):\n",
    "    if pop <= 2:\n",
    "        return '[1-2]'\n",
    "    elif pop <= 10:\n",
    "        return '[3-10]'\n",
    "    elif pop <= 50:\n",
    "        return '[11-50]'\n",
    "    elif pop <= 200:\n",
    "        return '[51-200]'\n",
    "    else:\n",
    "        return '[200+]'\n",
    "\n",
    "# add bucket to test_warm\n",
    "test_warm_with_pop = test_warm.copy()\n",
    "test_warm_with_pop['popularity'] = test_warm_with_pop['movieId'].map(\n",
    "    lambda x: item_popularity.get(x, 0)\n",
    ")\n",
    "test_warm_with_pop['bucket'] = test_warm_with_pop['popularity'].apply(get_popularity_bucket)\n",
    "\n",
    "# order buckets\n",
    "bucket_order = ['[1-2]', '[3-10]', '[11-50]', '[51-200]', '[200+]']\n",
    "\n",
    "# compute NDCG@10 for each bucket\n",
    "bucket_results = []\n",
    "\n",
    "for bucket in bucket_order:\n",
    "    bucket_test = test_warm_with_pop[test_warm_with_pop['bucket'] == bucket]\n",
    "    if len(bucket_test) < 100:  # Skip if too few samples\n",
    "        continue\n",
    "    \n",
    "    bucket_candidate_pool = warm_candidate_pool\n",
    "    bucket_users = bucket_test['userId'].unique()\n",
    "    \n",
    "    # sample users for efficiency\n",
    "    if len(bucket_users) > 1000:\n",
    "        bucket_users = np.random.choice(bucket_users, size=1000, replace=False)\n",
    "    \n",
    "    for model_name, predict_fn in models_warm.items():\n",
    "        # simple evaluation on bucket subset\n",
    "        bucket_test_sample = bucket_test[bucket_test['userId'].isin(bucket_users)]\n",
    "        \n",
    "        rank_metrics = evaluate_ranking(\n",
    "            predict_fn, train_warm, bucket_test_sample,\n",
    "            bucket_candidate_pool, K=K, N_neg=N_NEG, max_users=1000,\n",
    "            relevance_threshold=RELEVANCE_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        bucket_results.append({\n",
    "            'Bucket': bucket,\n",
    "            'Model': model_name,\n",
    "            f'NDCG@{K}': rank_metrics[f'NDCG@{K}'],\n",
    "            f'Recall@{K}': rank_metrics[f'Recall@{K}']\n",
    "        })\n",
    "\n",
    "bucket_results_df = pd.DataFrame(bucket_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name in ['CF', 'CBF']:\n",
    "    model_data = bucket_results_df[bucket_results_df['Model'] == model_name]\n",
    "    if len(model_data) > 0:\n",
    "        ax.plot(model_data['Bucket'], model_data[f'NDCG@{K}'], \n",
    "                marker='o', linewidth=2, markersize=8, label=model_name)\n",
    "\n",
    "ax.set_xlabel('Item Popularity Bucket', fontsize=12)\n",
    "ax.set_ylabel(f'NDCG@{K}', fontsize=12)\n",
    "ax.set_title('Warm Regime: Performance vs Item Popularity', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'warm_popularity_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlot saved to {FIGURES_DIR / 'warm_popularity_plot.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 REGIME COMPARISON BAR CHARTS (SEPARATE FIGURES)\n",
    "\n",
    "print()\n",
    "print(\"PLOT 8.2: Regime Comparison\")\n",
    "print()\n",
    "\n",
    "# combine all results\n",
    "all_results = pd.concat([\n",
    "    warm_results_df,\n",
    "    cold_results_df,\n",
    "    fewshot_results_df[fewshot_results_df['k'].isin([1, 3, 5])]\n",
    "], ignore_index=True)\n",
    "\n",
    "# create a simplified regime label\n",
    "def simplify_regime(row):\n",
    "    if 'Warm' in row['Regime']:\n",
    "        return 'Warm'\n",
    "    elif 'Cold' in row['Regime']:\n",
    "        return 'Cold-Item'\n",
    "    else:\n",
    "        return row['Regime']\n",
    "\n",
    "all_results['Regime_Simple'] = all_results.apply(simplify_regime, axis=1)\n",
    "\n",
    "# prepare data for plotting\n",
    "regimes_order = ['Warm', 'Cold-Item', 'Few-shot k=1', 'Few-shot k=3', 'Few-shot k=5']\n",
    "models_order = ['CF', 'CBF', 'Hybrid', 'CF_fallback']\n",
    "colors = {'CF': '#2ecc71', 'CBF': '#3498db', 'Hybrid': '#e74c3c', 'CF_fallback': '#95a5a6'}\n",
    "\n",
    "x = np.arange(len(regimes_order))\n",
    "width = 0.25\n",
    "\n",
    "# helper function to get values for a metric\n",
    "def get_metric_values(metric_col, model):\n",
    "    values = []\n",
    "    for regime in regimes_order:\n",
    "        if regime == 'Cold-Item' and model == 'CF':\n",
    "            # use CF_fallback for cold regime\n",
    "            model_data = all_results[(all_results['Regime'] == regime) & (all_results['Model'] == 'CF_fallback')]\n",
    "        else:\n",
    "            model_data = all_results[(all_results['Regime'] == regime) & (all_results['Model'] == model)]\n",
    "        \n",
    "        if len(model_data) > 0:\n",
    "            values.append(model_data[metric_col].values[0])\n",
    "        else:\n",
    "            values.append(0)\n",
    "    return values\n",
    "\n",
    "# FIGURE 1: Recall@K\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "recall_col = f'Recall@{K}'\n",
    "\n",
    "multiplier = 0\n",
    "for model in ['CF', 'CBF', 'Hybrid']:\n",
    "    values = get_metric_values(recall_col, model)\n",
    "    offset = width * multiplier\n",
    "    ax1.bar(x + offset, values, width, label=model, color=colors.get(model, '#333'))\n",
    "    multiplier += 1\n",
    "\n",
    "ax1.set_xlabel('Regime', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel(recall_col, fontsize=13, fontweight='bold')\n",
    "ax1.set_title(f'{recall_col} Comparison Across Regimes', fontsize=15, fontweight='bold')\n",
    "ax1.set_xticks(x + width)\n",
    "ax1.set_xticklabels(regimes_order, rotation=15, ha='right', fontsize=11)\n",
    "ax1.legend(fontsize=11, loc='upper right')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim(0, max([max(get_metric_values(recall_col, m)) for m in ['CF', 'CBF', 'Hybrid']]) * 1.15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'regime_comparison_recall.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'regime_comparison_recall.png'}\")\n",
    "\n",
    "# FIGURE 2: NDCG@K\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "ndcg_col = f'NDCG@{K}'\n",
    "\n",
    "multiplier = 0\n",
    "for model in ['CF', 'CBF', 'Hybrid']:\n",
    "    values = get_metric_values(ndcg_col, model)\n",
    "    offset = width * multiplier\n",
    "    ax2.bar(x + offset, values, width, label=model, color=colors.get(model, '#333'))\n",
    "    multiplier += 1\n",
    "\n",
    "ax2.set_xlabel('Regime', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel(ndcg_col, fontsize=13, fontweight='bold')\n",
    "ax2.set_title(f'{ndcg_col} Comparison Across Regimes', fontsize=15, fontweight='bold')\n",
    "ax2.set_xticks(x + width)\n",
    "ax2.set_xticklabels(regimes_order, rotation=15, ha='right', fontsize=11)\n",
    "ax2.legend(fontsize=11, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_ylim(0, max([max(get_metric_values(ndcg_col, m)) for m in ['CF', 'CBF', 'Hybrid']]) * 1.15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'regime_comparison_ndcg.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'regime_comparison_ndcg.png'}\")\n",
    "\n",
    "# FIGURE 3: Precision@K\n",
    "fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "precision_col = f'Precision@{K}'\n",
    "\n",
    "multiplier = 0\n",
    "for model in ['CF', 'CBF', 'Hybrid']:\n",
    "    values = get_metric_values(precision_col, model)\n",
    "    offset = width * multiplier\n",
    "    ax3.bar(x + offset, values, width, label=model, color=colors.get(model, '#333'))\n",
    "    multiplier += 1\n",
    "\n",
    "ax3.set_xlabel('Regime', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylabel(precision_col, fontsize=13, fontweight='bold')\n",
    "ax3.set_title(f'{precision_col} Comparison Across Regimes', fontsize=15, fontweight='bold')\n",
    "ax3.set_xticks(x + width)\n",
    "ax3.set_xticklabels(regimes_order, rotation=15, ha='right', fontsize=11)\n",
    "ax3.legend(fontsize=11, loc='upper right')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "ax3.set_ylim(0, .5)  # Precision is between 0 and 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'regime_comparison_precision.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'regime_comparison_precision.png'}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All regime comparison plots saved!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary and Key Findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. FINAL SUMMARY\n",
    "\n",
    "print()\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print()\n",
    "\n",
    "# list all saved files\n",
    "print(\"\\n📁 SAVED FILES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tables_files = list(TABLES_DIR.glob('*.csv'))\n",
    "figures_files = list(FIGURES_DIR.glob('*.png'))\n",
    "\n",
    "print(\"\\n📊 Tables:\")\n",
    "for f in sorted(tables_files):\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(\"\\n📈 Figures:\")\n",
    "for f in sorted(figures_files):\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(\"\\n💾 Cache files (for faster re-runs):\")\n",
    "cache_files = list(CACHE_DIR.glob('*'))\n",
    "for f in sorted(cache_files)[:10]:  # Show first 10\n",
    "    print(f\"  - {f}\")\n",
    "if len(cache_files) > 10:\n",
    "    print(f\"  ... and {len(cache_files) - 10} more\")\n",
    "\n",
    "# key findings\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print()\n",
    "\n",
    "print(f\"\\n1️⃣  BEST EMBEDDING MODEL: {best_embedding_model}\")\n",
    "print(f\"    Selected based on CBF performance on warm validation set\")\n",
    "\n",
    "print(f\"\\n2️⃣  HYBRID STRATEGY: switching (No Blending)\")\n",
    "print(f\"    Warm items → CF ratings (best for rating prediction)\")\n",
    "print(f\"    Cold items → CBF similarity (only model that works)\")\n",
    "print(f\"    No alpha parameter - simple gating based on item warmth\")\n",
    "\n",
    "# find winners for each regime\n",
    "print(\"\\n3️⃣  WINNERS BY REGIME (based on NDCG@10):\")\n",
    "\n",
    "# warm winner\n",
    "warm_winner = warm_results_df.loc[warm_results_df[f'NDCG@{K}'].idxmax(), 'Model']\n",
    "warm_best_ndcg = warm_results_df[f'NDCG@{K}'].max()\n",
    "print(f\"    - Warm: {warm_winner} (NDCG@{K}={warm_best_ndcg:.4f})\")\n",
    "\n",
    "# cold winner\n",
    "cold_winner = cold_results_df.loc[cold_results_df[f'NDCG@{K}'].idxmax(), 'Model']\n",
    "cold_best_ndcg = cold_results_df[f'NDCG@{K}'].max()\n",
    "print(f\"    - Cold-Item: {cold_winner} (NDCG@{K}={cold_best_ndcg:.4f})\")\n",
    "\n",
    "# few-shot winners\n",
    "for k in [1, 3, 5]:\n",
    "    fs_df = fewshot_results_df[fewshot_results_df['k'] == k]\n",
    "    if len(fs_df) > 0:\n",
    "        fs_winner = fs_df.loc[fs_df[f'NDCG@{K}'].idxmax(), 'Model']\n",
    "        fs_best_ndcg = fs_df[f'NDCG@{K}'].max()\n",
    "        print(f\"    - Few-shot k={k}: {fs_winner} (NDCG@{K}={fs_best_ndcg:.4f})\")\n",
    "\n",
    "# hybrid robustness\n",
    "print(\"\\n4️⃣  HYBRID MODEL ROBUSTNESS:\")\n",
    "print(\"    The gated hybrid approach provides:\")\n",
    "print(\"    - Competitive performance on warm items (benefiting from CF)\")\n",
    "print(\"    - Graceful degradation on cold items (fallback to CBF)\")\n",
    "print(\"    - Consistent results across few-shot scenarios\")\n",
    "\n",
    "# performance patterns\n",
    "print(\"\\n5️⃣  PERFORMANCE PATTERNS:\")\n",
    "print(\"    - CF excels on warm items with rich interaction history\")\n",
    "print(\"    - CBF provides crucial coverage for cold items\")\n",
    "print(\"    - Performance degrades from warm → cold → few-shot\")\n",
    "print(\"    - Hybrid balances both approaches effectively\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
    "print()\n",
    "print(f\"\\nTotal tables saved: {len(tables_files)}\")\n",
    "print(f\"Total figures saved: {len(figures_files)}\")\n",
    "print(f\"Total cache files: {len(cache_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINED RESULTS TABLE (Paper-Ready)\n",
    "\n",
    "print()\n",
    "print(\"COMBINED RESULTS TABLE (Paper-Ready)\")\n",
    "print()\n",
    "\n",
    "# combine all results into one DataFrame\n",
    "combined_results = pd.concat([\n",
    "    warm_results_df.assign(Regime_Order=1),\n",
    "    cold_results_df.assign(Regime_Order=2),\n",
    "    fewshot_results_df.assign(Regime_Order=3)\n",
    "], ignore_index=True)\n",
    "\n",
    "# select and order columns\n",
    "display_cols = ['Regime', 'Model', 'MAE', 'RMSE', f'Recall@{K}', f'NDCG@{K}', f'Precision@{K}']\n",
    "combined_display = combined_results[display_cols].copy()\n",
    "\n",
    "# format numeric columns\n",
    "for col in ['MAE', 'RMSE', f'Recall@{K}', f'NDCG@{K}', f'Precision@{K}']:\n",
    "    combined_display[col] = combined_display[col].apply(lambda x: f'{x:.4f}')\n",
    "\n",
    "print(\"\\n\" + combined_display.to_string(index=False))\n",
    "\n",
    "# save combined results\n",
    "combined_results[display_cols].to_csv(TABLES_DIR / 'combined_results.csv', index=False)\n",
    "print(f\"\\nCombined results saved to {TABLES_DIR / 'combined_results.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Qualitative Analysis: Example Users and Recommendations\n",
    "\n",
    "Show concrete examples of what each model recommends for real users across different regimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qualitative analysis helper\n",
    "\n",
    "movies_with_titles = pd.read_csv(DATA_DIR / 'movies_enriched_big.csv', \n",
    "                                  usecols=['movieId', 'title'])\n",
    "print(f\"loaded {len(movies_with_titles):,} movie titles\")\n",
    "\n",
    "# shows recommendations for a user and compares with actual test ratings\n",
    "def show_user_recommendations(user_id, train_df, test_df, models_dict, movies_df, \n",
    "                               regime_name, top_k=10, show_train_sample=5):\n",
    "    # get user's training and test items\n",
    "    user_train = train_df[train_df['userId'] == user_id].sort_values('rating', ascending=False)\n",
    "    user_test = test_df[test_df['userId'] == user_id].sort_values('rating', ascending=False)\n",
    "    \n",
    "    if len(user_test) == 0:\n",
    "        print(f\"user {user_id} has no test data, skipping\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nuser {user_id} - {regime_name} regime\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # training history sample\n",
    "    print(f\"\\ntraining history ({len(user_train)} items, top {show_train_sample} by rating):\")\n",
    "    for idx, row in user_train.head(show_train_sample).iterrows():\n",
    "        movie_info = movies_with_titles[movies_with_titles['movieId'] == row['movieId']]\n",
    "        if len(movie_info) > 0:\n",
    "            title = movie_info.iloc[0]['title']\n",
    "            print(f\"  {row['rating']:.1f} - {title}\")\n",
    "    \n",
    "    # test ground truth (highly rated)\n",
    "    print(f\"\\ntest ground truth ({len(user_test)} items, showing ratings >= 4.0):\")\n",
    "\n",
    "    high_rated_test = user_test[user_test['rating'] >= 4.0]\n",
    "    if len(high_rated_test) > 0:\n",
    "        for idx, row in high_rated_test.head(10).iterrows():\n",
    "            movie_info = movies_with_titles[movies_with_titles['movieId'] == row['movieId']]\n",
    "            if len(movie_info) > 0:\n",
    "                title = movie_info.iloc[0]['title']\n",
    "                print(f\"  {row['rating']:.1f} - {title}\")\n",
    "    else:\n",
    "        print(\"  (no ratings >= 4.0)\")\n",
    "    \n",
    "    # candidate items are test items not in training\n",
    "    user_train_items = set(user_train['movieId'].values)\n",
    "    user_test_items = set(user_test['movieId'].values)\n",
    "    candidate_items = list(user_test_items - user_train_items)\n",
    "    \n",
    "    if len(candidate_items) == 0:\n",
    "        print(\"\\nno new items to recommend\")\n",
    "        return\n",
    "    \n",
    "    # prepare pairs for scoring\n",
    "    pairs_df = pd.DataFrame({\n",
    "        'userId': [user_id] * len(candidate_items),\n",
    "        'movieId': candidate_items\n",
    "    })\n",
    "    \n",
    "    # get actual ratings\n",
    "    actual_ratings = test_df[\n",
    "        (test_df['userId'] == user_id) & \n",
    "        (test_df['movieId'].isin(candidate_items))\n",
    "    ].set_index('movieId')['rating'].to_dict()\n",
    "    \n",
    "    # show recommendations from each model\n",
    "    for model_name, predict_fn in models_dict.items():\n",
    "        print(f\"\\n{model_name.upper()} recommendations (top {top_k}):\")\n",
    "        \n",
    "        scores = predict_fn(pairs_df)\n",
    "        ranked_indices = np.argsort(-scores)\n",
    "        \n",
    "        # display top-k\n",
    "        for i, idx in enumerate(ranked_indices[:top_k]):\n",
    "            movie_id = candidate_items[idx]\n",
    "            pred_score = scores[idx]\n",
    "            actual_rating = actual_ratings.get(movie_id, None)\n",
    "            \n",
    "            movie_info = movies_with_titles[movies_with_titles['movieId'] == movie_id]\n",
    "            if len(movie_info) > 0:\n",
    "                title = movie_info.iloc[0]['title']\n",
    "                \n",
    "                if actual_rating is not None:\n",
    "                    match_symbol = \"+\" if actual_rating >= 4.0 else \"-\"\n",
    "                    print(f\"  {i+1:2d}. {title[:60]:60s} | pred: {pred_score:.2f} | actual: {actual_rating:.1f} {match_symbol}\")\n",
    "                else:\n",
    "                    print(f\"  {i+1:2d}. {title[:60]:60s} | pred: {pred_score:.2f} | actual: n/a\")\n",
    "        \n",
    "        # compute precision for this user\n",
    "        top_k_items = [candidate_items[idx] for idx in ranked_indices[:top_k]]\n",
    "        hits = [item for item in top_k_items if actual_ratings.get(item, 0) >= 4.0]\n",
    "        precision = len(hits) / top_k if top_k > 0 else 0\n",
    "        print(f\"\\n  precision@{top_k} (>=4.0): {len(hits)}/{top_k} = {precision:.2%}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"helper function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Warm Regime Examples\n",
    "\n",
    "Show examples of recommendations for users in the warm regime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 WARM REGIME EXAMPLES\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"WARM REGIME: Qualitative Examples\")\n",
    "print()\n",
    "\n",
    "# select a few interesting users (with diverse preferences)\n",
    "# pick users with reasonable amount of test data\n",
    "test_user_counts = test_warm.groupby('userId').size()\n",
    "candidate_users = test_user_counts[test_user_counts >= 20].index.tolist()\n",
    "\n",
    "# sample 2 users for detailed analysis\n",
    "np.random.seed(SEED)\n",
    "example_users_warm = np.random.choice(candidate_users, size=2, replace=False)\n",
    "\n",
    "# models to compare\n",
    "# note: CBF shows similarity scores (not ratings)\n",
    "models_warm_qual = {\n",
    "    'CF (ratings)': cf_warm.predict,\n",
    "    'CBF (similarity)': cbf_warm.predict,  # Returns similarity scores\n",
    "    'Hybrid (ratings)': hybrid_warm.predict\n",
    "}\n",
    "\n",
    "print(\"note: CBF shows SIMILARITY SCORES (not ratings)\")\n",
    "print(\"Higher similarity = more relevant. Scores typically range from 0 to 1.\")\n",
    "print()\n",
    "\n",
    "# show examples\n",
    "for user_id in example_users_warm:\n",
    "    show_user_recommendations(\n",
    "        user_id=user_id,\n",
    "        train_df=train_warm,\n",
    "        test_df=test_warm,\n",
    "        models_dict=models_warm_qual,\n",
    "        movies_df=movies_with_titles,\n",
    "        regime_name=\"Warm\",\n",
    "        top_k=10,\n",
    "        show_train_sample=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Cold-Start Item Regime Examples\n",
    "\n",
    "Show examples of recommendations for cold items (items that didn't exist in training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 COLD-START ITEM REGIME EXAMPLES\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COLD-ITEM REGIME: Qualitative Examples\")\n",
    "print()\n",
    "\n",
    "# select users who rated cold items AND have training data (for CBF profile)\n",
    "train_cold_users = set(train_cold['userId'].unique())\n",
    "test_cold_user_counts = test_cold.groupby('userId').size()\n",
    "\n",
    "# IMPORTANT: User must be in train_cold to have a CBF profile\n",
    "# select users with MORE test items (> =20) for better qualitative analysis\n",
    "candidate_users_cold = [\n",
    "    uid for uid, count in test_cold_user_counts.items() \n",
    "    if count >= 20 and uid in train_cold_users  # Increased from 10 to 20\n",
    "]\n",
    "\n",
    "print(f\"Candidate users (≥20 test items, in both train & test): {len(candidate_users_cold)}\")\n",
    "\n",
    "if len(candidate_users_cold) < 2:\n",
    "    print(\"warning: Not enough users with ≥20 test items, lowering threshold to ≥15\")\n",
    "    candidate_users_cold = [\n",
    "        uid for uid, count in test_cold_user_counts.items() \n",
    "        if count >= 15 and uid in train_cold_users\n",
    "    ]\n",
    "    print(f\"Candidate users (≥15 test items): {len(candidate_users_cold)}\")\n",
    "\n",
    "# sample 2 users\n",
    "np.random.seed(SEED + 1)\n",
    "example_users_cold = np.random.choice(candidate_users_cold, size=2, replace=False)\n",
    "\n",
    "# print selected user test counts\n",
    "for uid in example_users_cold:\n",
    "    n_test = len(test_cold[test_cold['userId'] == uid])\n",
    "    print(f\"  User {uid}: {n_test} cold items in test set\")\n",
    "\n",
    "# models to compare (CF_fallback instead of regular CF)\n",
    "# for cold-start qualitative analysis:\n",
    "# - CF_fallback: uses user mean (for reference)\n",
    "# - CBF: uses SIMILARITY SCORES (not ratings)\n",
    "# - Hybrid: uses CBF similarity scores for cold items\n",
    "\n",
    "models_cold_qual = {\n",
    "    'CF_fallback (ratings)': cf_cold.predict_fallback,\n",
    "    'CBF (similarity)': cbf_cold.predict,  # CBF.predict returns similarity scores\n",
    "    'Hybrid (similarity)': cbf_cold.predict  # Same as CBF for cold items\n",
    "}\n",
    "\n",
    "print(\"note: CBF and Hybrid show SIMILARITY SCORES (not ratings) for cold-start\")\n",
    "print(\"Higher similarity = more relevant. Scores typically range from 0 to 1.\")\n",
    "print()\n",
    "\n",
    "# show examples\n",
    "for user_id in example_users_cold:\n",
    "    show_user_recommendations(\n",
    "        user_id=user_id,\n",
    "        train_df=train_cold,\n",
    "        test_df=test_cold,\n",
    "        models_dict=models_cold_qual,\n",
    "        movies_df=movies_with_titles,\n",
    "        regime_name=\"Cold-Item\",\n",
    "        top_k=10,\n",
    "        show_train_sample=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Few-Shot User Regime Examples\n",
    "\n",
    "Show examples of recommendations for users with limited training data (k=1, 3, 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4 FEW-SHOT USER REGIME EXAMPLES\n",
    "\n",
    "# show examples for k=1, k=3, and k=5\n",
    "for k in [1, 3, 5]:\n",
    "    print(f\"# FEW-SHOT k={k}: Users with only {k} training rating(s)\")\n",
    "    \n",
    "    train_k, test_k = fewshot_splits[k]\n",
    "    \n",
    "    # select users with reasonable test data\n",
    "    test_fewshot_counts = test_k.groupby('userId').size()\n",
    "    candidate_users_fewshot = test_fewshot_counts[test_fewshot_counts >= 20].index.tolist()\n",
    "    \n",
    "    # sample 1 user per k (to keep output manageable)\n",
    "    np.random.seed(SEED + k)\n",
    "    example_user = np.random.choice(candidate_users_fewshot, size=1)[0]\n",
    "    \n",
    "    # need to re-train models for this k (or use cached if available)\n",
    "    # for qualitative analysis, we'll use the last trained models from the few-shot experiments\n",
    "    # note: This assumes the few-shot experiment loop has completed\n",
    "    \n",
    "    print(f\"note: Using models trained on few-shot k={k} data from earlier experiments.\")\n",
    "    print(f\"training set size: {len(train_k):,} ratings\")\n",
    "    print(f\"Test set size: {len(test_k):,} ratings\\n\")\n",
    "    \n",
    "    # for demonstration, we'll train quick models here\n",
    "    # CF model\n",
    "    cf_k = CFModel(n_factors=50, n_epochs=15, random_state=SEED)\n",
    "    cf_k.fit(train_k, verbose=False)\n",
    "    \n",
    "    # CBF model (use cached profile if available)\n",
    "    cbf_k = CBFModel(use_rating_weights=True)\n",
    "    cbf_k.fit(train_k, best_embeddings, best_movieId_to_idx, verbose=False)\n",
    "    \n",
    "    # switching hybrid\n",
    "    hybrid_k = HybridModel(cf_k, cbf_k)\n",
    "    \n",
    "    # models dict\n",
    "    # note: CBF shows similarity scores (not ratings)\n",
    "    models_fewshot_qual = {\n",
    "        'CF (ratings)': cf_k.predict,\n",
    "        'CBF (similarity)': cbf_k.predict,  # Returns similarity scores\n",
    "        'Hybrid (ratings)': hybrid_k.predict\n",
    "    }\n",
    "    \n",
    "    if k == 1:  # Only print once\n",
    "        print(\"note: CBF shows SIMILARITY SCORES (not ratings)\")\n",
    "        print(\"Higher similarity = more relevant. Scores typically range from 0 to 1.\")\n",
    "        print()\n",
    "    \n",
    "    # show recommendation\n",
    "    show_user_recommendations(\n",
    "        user_id=example_user,\n",
    "        train_df=train_k,\n",
    "        test_df=test_k,\n",
    "        models_dict=models_fewshot_qual,\n",
    "        movies_df=movies_with_titles,\n",
    "        regime_name=f\"Few-shot k={k}\",\n",
    "        top_k=10,\n",
    "        show_train_sample=k  # Show all k training examples\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
